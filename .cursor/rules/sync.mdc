---
alwaysApply: false
---

# AI Core 同步机制与多缓冲流水设计

## 1. AI Core 硬件架构概述

AI Core中包含计算单元、存储单元、搬运单元等核心组件。

### 1.1 核心组件

| 组件分类 | 组件名称 | 组件功能 |
|---------|---------|---------|
| 计算单元 | Scalar | 执行地址计算、循环控制等标量计算工作，并把向量计算、矩阵计算、数据搬运、同步指令发射给对应单元执行 |
| 计算单元 | Vector | 负责执行向量运算 |
| 计算单元 | Cube | 负责执行矩阵运算 |
| 存储单元 | Local Memory | AI Core的内部存储，对应的数据类型为LocalTensor |
| 存储单元 | Global Memory | AI Core能够访问的外部存储，对应的数据类型为GlobalTensor |
| 搬运单元 | DMA | 负责数据搬运，包括Global Memory和Local Memory之间的数据搬运，以及不同层级Local Memory之间的数据搬运 |

### 1.2 三个关键过程

开发者在理解硬件架构的抽象时，需要重点关注如下异步指令流、同步信号流、计算数据流三个过程：

1. **异步指令流**: Scalar计算单元读取指令序列，并把向量计算、矩阵计算、数据搬运指令发射给对应单元的指令队列，向量计算单元、矩阵计算单元、数据搬运单元异步的并行执行接收到的指令。

2. **同步信号流**: 不同的指令间有可能存在依赖关系，为了保证不同指令队列间的指令按照正确的逻辑关系执行，Scalar计算单元也会给对应单元下发同步指令。

3. **计算数据流**: DMA搬入单元将数据从Global Memory搬运到Local Memory，Vector/Cube计算单元完成数据计算，并把计算结果写回Local Memory，DMA搬出单元把处理好的数据从Local Memory搬运回Global Memory。

四个执行单元Scalar、Vector、DMA(VECIN)、DMA(VECOUT)并行执行，若访问同一片Local Memory，需要同步机制来控制他们的访问时序：保证先搬入Local Memory后再计算，计算完成后再搬出。

## 2. 流水线类型

| 流水类型 | 含义 |
|---------|------|
| PIPE_S | 标量流水线，使用Tensor GetValue函数时为此流水 |
| PIPE_V | 矢量计算流水及L0C->UB数据搬运流水、UB->UB的搬运指令 |
| PIPE_M | 矩阵计算流水 |
| PIPE_MTE1 | L1->L0A、L1->L0B数据搬运流水 |
| PIPE_MTE2 | GM->L1、GM->L0A、GM->L0B、GM->UB数据搬运流水 |
| PIPE_MTE3 | UB->GM、UB->L1数据搬运流水 |
| PIPE_FIX | L0C->GM、L0C->L1数据搬运流水 |

## 3. 同步控制机制

### 3.1 同步控制分类

对上述并行流水的同步控制分为两种：

1. **多流水同步**: 通过TQueSync的SetFlag/WaitFlag或者SetFlag/WaitFlag(ISASI)接口进行不同流水线间的同步控制。
   - **SetFlag**: 当前序指令的所有读写操作都完成之后，当前指令开始执行，并将硬件中的对应标志位设置为1。
   - **WaitFlag**: 当执行到该指令时，如果发现对应标志位为0，该队列的后续指令将一直被阻塞；如果发现对应标志位为1，则将对应标志位设置为0，同时后续指令开始执行。

2. **单流水同步**: 通过PipeBarrier(ISASI)完成同一流水线内的同步控制，用于在同一流水线内部约束执行顺序。其作用是，保证前序指令中所有数据的读写工作全部完成，后序指令才能执行。

### 3.2 同步事件语义

**X_Y表示Y等X**:
- `V_MTE2`: MTE2流水线等待V流水线完成
- `MTE2_V`: V流水线等待MTE2流水线完成
- `V_MTE3`: MTE3流水线等待V流水线完成
- `MTE3_V`: V流水线等待MTE3流水线完成

**示例**:
```cpp
// 同步 V->MTE3: 向量计算完成
AscendC::SetFlag<AscendC::HardEvent::V_MTE3>(EVENT_ID0); 
// 此处理解成，Set前方的V指令如果未完成，则flag0为0，完成设为1。(X_Y表示Y等X)

AscendC::WaitFlag<AscendC::HardEvent::V_MTE3>(EVENT_ID0); 
// 此处理解成，Wait后方的MTE3指令会被阻塞执行，直到flag0设为1。
```

## 4. Double Buffer 机制

### 4.1 问题背景

以纯Vector计算为例，矢量计算前后的CopyIn、CopyOut过程使用搬运指令队列（MTE2/MTE3），Compute过程使用Vector指令队列（V），不同指令队列可并行执行，意味着CopyIn、CopyOut过程和Compute过程是可以并行的。

在此过程中，数据搬运与Vector计算串行执行，Vector计算单元不可避免存在资源闲置问题，假设CopyIn、Compute、CopyOut三阶段分别耗时相同均为t，则Vector的利用率仅为1/3，等待时间过长，Vector利用率严重不足。

### 4.2 Double Buffer 解决方案

为减少Vector等待时间，使能double buffer机制将待处理的数据一分为二，比如Tensor1、Tensor2。当Vector单元对Tensor1中数据进行Compute计算时，Tensor2数据流可以执行CopyIn的过程；而当Vector切换到计算Tensor2时，Tensor1数据流可以执行CopyOut的过程。由此，数据的进出搬运和Vector计算实现并行执行，Vector闲置问题得以有效缓解。

### 4.3 同步控制策略

#### 正向同步
在本次数据搬入和计算之间，插入MTE2_V（矢量计算流水等待MT2搬运流水）同步事件，确保数据搬入之后再进行计算；在本次数据计算和搬出之间，插入V_MTE3（MTE3搬运流水等待矢量计算流水）同步事件，确保数据计算完成后再进行搬出。

#### 反向同步
在上一次的数据计算和本次数据搬入之间，插入V_MTE2（MT2搬运流水等待矢量计算流水）同步事件，确保上一次的数据计算完成后，本次的数据再进行搬入。防止本次的数据会覆盖掉上一次未计算完成的数据；在上一次的数据搬出和本次数据计算之间，插入MTE3_V（矢量计算流水等待MT3搬运流水）同步事件，确保上一次的数据搬出后，再进行本次数据的计算。防止本次的数据会覆盖掉上一次未搬出的数据。

## 5. 多缓冲流水机制通用逻辑

### 5.1 多缓冲机制扩展

基于double buffer机制，可以扩展到N-buffer实现更高效的流水并行：

- **UB_STAGES参数**: 控制buffer数量，实现多级流水
- **Buffer轮转**: 通过`ubListId`循环使用不同buffer，实现数据搬运与计算的完全并行
- **内存布局**: 每个stage的buffer在UB中连续分配，支持多级流水

### 5.2 生产者-消费者同步模式

#### 输入数据路径 (GM → UB → V计算)
```
GM --[MTE2]--> ubBuffer --[V操作]--> 临时buffer
```

**同步逻辑**:
```cpp
// 等待数据搬入完成
AscendC::WaitFlag<AscendC::HardEvent::MTE2_V>(eventId);
// 执行V流水操作
AscendC::Cast(ubFp32, ubBuffer, ...);
// 通知MTE2可以覆盖这块buffer
AscendC::SetFlag<AscendC::HardEvent::V_MTE2>(eventId);
```

**含义**: 等待MTE2搬入完成 → V消费数据 → 告诉MTE2"可以覆盖了"

#### 输出数据路径 (V计算 → UB → GM)
```
计算结果 --[V操作]--> ubBuffer --[MTE3]--> GM
```

**同步逻辑**:
```cpp
// 等待buffer空闲(上一轮MTE3搬出完成)
AscendC::WaitFlag<AscendC::HardEvent::MTE3_V>(eventId);
// 执行V流水操作
AscendC::Cast(ubBuffer, result, ...);
// 通知MTE3可以搬出数据
AscendC::SetFlag<AscendC::HardEvent::V_MTE3>(eventId);
```

**含义**: 等待buffer空闲 → V生产数据 → 告诉MTE3"可以搬走了"

### 5.3 同步事件方向判断规则

**关键原则**: 根据数据流向确定同步事件类型

| 操作类型 | 数据流向 | Wait事件 | Set事件 | 含义 |
|---------|---------|----------|---------|------|
| 读buffer(消费) | MTE2 → V | `MTE2_V` | `V_MTE2` | "我读完了，你可以覆盖" |
| 写buffer(生产) | V → MTE3 | `MTE3_V` | `V_MTE3` | "我写好了，你可以搬走" |

**判断方法**:
- 如果V在**消费**buffer数据 → Set `V_MTE2` (告诉生产者可以覆盖)
- 如果V在**生产**buffer数据 → Set `V_MTE3` (告诉消费者可以使用)

### 5.4 事件ID分配与初始化逻辑

#### 事件ID分配策略
```cpp
// 为每个buffer stage分配独立的事件ID
int32_t eventVMTE2 = 0;      // V->MTE2事件计数器
int32_t eventMTE2V = 0;      // MTE2->V事件计数器  
int32_t eventMTE3V = 0;      // MTE3->V事件计数器
int32_t eventVMTE3 = 0;      // V->MTE3事件计数器

for (uint32_t i = 0; i < UB_STAGES; ++i) {
    // 输入buffer事件分配
    eventUbCVMTE2List[i] = eventVMTE2++;           // V->MTE2: ubC
    eventUbCMTE2VList[i] = eventMTE2V++;          // MTE2->V: ubC
    eventUbScaleVMTE2List[i] = eventVMTE2++;       // V->MTE2: ubScale
    eventUbScaleMTE2VList[i] = eventMTE2V++;       // MTE2->V: ubScale
    eventUbPerTokenScaleVMTE2List[i] = eventVMTE2++; // V->MTE2: ubPerTokenScale
    eventUbPerTokenScaleMTE2VList[i] = eventMTE2V++; // MTE2->V: ubPerTokenScale
    
    // 输出buffer事件分配
    eventUbDMTE3VList[i] = eventMTE3V++;           // MTE3->V: ubD
    eventUbDVMTE3List[i] = eventVMTE3++;           // V->MTE3: ubD
}
```

#### 初始化阶段Flag设置
```cpp
// 为每个buffer stage设置初始flag状态
for (uint32_t i = 0; i < UB_STAGES; ++i) {
    // 输入buffer初始状态: V已完成，允许MTE2开始搬入
    AscendC::SetFlag<AscendC::HardEvent::V_MTE2>(eventUbCVMTE2List[i]);
    AscendC::SetFlag<AscendC::HardEvent::V_MTE2>(eventUbScaleVMTE2List[i]);
    AscendC::SetFlag<AscendC::HardEvent::V_MTE2>(eventUbPerTokenScaleVMTE2List[i]);
    
    // 输出buffer初始状态: MTE3已完成，允许V开始写入
    AscendC::SetFlag<AscendC::HardEvent::MTE3_V>(eventUbDMTE3VList[i]);
}
```

### 5.5 实际代码示例

#### 循环中的同步模式
```cpp
// 输入数据: 三个独立的buffer并行处理
// ubC处理
AscendC::WaitFlag<AscendC::HardEvent::V_MTE2>(eventUbCVMTE2List[ubListId]);
copyGmToUbC(ubC, gmTileC, ...);  // MTE2搬入
AscendC::SetFlag<AscendC::HardEvent::MTE2_V>(eventUbCMTE2VList[ubListId]);

AscendC::WaitFlag<AscendC::HardEvent::MTE2_V>(eventUbCMTE2VList[ubListId]);
AscendC::Cast(ubCFp32, ubC, ...);  // V消费数据
AscendC::SetFlag<AscendC::HardEvent::V_MTE2>(eventUbCVMTE2List[ubListId]);

// ubScale处理
AscendC::WaitFlag<AscendC::HardEvent::V_MTE2>(eventUbScaleVMTE2List[ubListId]);
copyGmToUbScale(ubScale, gmTileScale, ...);  // MTE2搬入
AscendC::SetFlag<AscendC::HardEvent::MTE2_V>(eventUbScaleMTE2VList[ubListId]);

AscendC::WaitFlag<AscendC::HardEvent::MTE2_V>(eventUbScaleMTE2VList[ubListId]);
AscendC::Cast(ubScaleFp32, ubScale, ...);  // V消费数据
AscendC::SetFlag<AscendC::HardEvent::V_MTE2>(eventUbScaleVMTE2List[ubListId]);

// ubPerTokenScale处理
AscendC::WaitFlag<AscendC::HardEvent::V_MTE2>(eventUbPerTokenScaleVMTE2List[ubListId]);
copyGmToUbPerTokenScale(ubPerTokenScale, gmTilePerTokenScale, ...);  // MTE2搬入
AscendC::SetFlag<AscendC::HardEvent::MTE2_V>(eventUbPerTokenScaleMTE2VList[ubListId]);

AscendC::WaitFlag<AscendC::HardEvent::MTE2_V>(eventUbPerTokenScaleMTE2VList[ubListId]);
AscendC::Cast(ubPerTokenScaleFp32, ubPerTokenScale, ...);  // V消费数据
AscendC::SetFlag<AscendC::HardEvent::V_MTE2>(eventUbPerTokenScaleVMTE2List[ubListId]);

// 计算阶段
AscendC::PipeBarrier<PIPE_V>();
tileRowBroadcastMul(ubMul, ubCFp32, ubScaleFp32);
tileBroadcastOneBlk(ubPerTokenScaleFp32Brcb, ubPerTokenScaleFp32);
AscendC::PipeBarrier<PIPE_V>();
tileOneBlkColumnBroadcastMul(ubPerTokenMul, ubMul, ubPerTokenScaleFp32Brcb);
AscendC::PipeBarrier<PIPE_V>();

// 输出数据: V生产数据
AscendC::WaitFlag<AscendC::HardEvent::MTE3_V>(eventUbDMTE3VList[ubListId]);
AscendC::Cast(ubD, ubPerTokenMul, ...);  // V生产数据
AscendC::SetFlag<AscendC::HardEvent::V_MTE3>(eventUbDVMTE3List[ubListId]);

// 搬出数据
AscendC::WaitFlag<AscendC::HardEvent::V_MTE3>(eventUbDVMTE3List[ubListId]);
copyUbToGmD(gmTileD, ubD, ...);  // MTE3搬出
AscendC::SetFlag<AscendC::HardEvent::MTE3_V>(eventUbDMTE3VList[ubListId]);

// Buffer轮转
ubListId = (ubListId + 1 < UB_STAGES) ? (ubListId + 1) : 0;
```

### 5.6 关键要点总结

1. **数据依赖决定同步方向**: 虽然都在同一流水线(如V)，但数据依赖关系决定同步事件的方向
2. **多缓冲提升并行度**: 通过N个buffer实现数据搬运与计算的完全并行，隐藏数据搬运延迟
3. **正确的初始状态**: 初始flag状态对首次迭代至关重要，确保流水能正确启动
4. **生产者-消费者模式**: 输入路径是消费者模式，输出路径是生产者模式
5. **事件语义**: `X_Y`表示Y等待X，`SetFlag<X_Y>`表示X完成，Y可以继续
6. **事件ID管理**: 每个buffer stage需要独立的事件ID，避免不同stage间的冲突
7. **Buffer轮转机制**: 通过`ubListId`实现多级流水的buffer复用

这种多缓冲流水机制实现了MTE2、V、MTE3三个执行单元的最大化并行，显著提升了AI Core的计算效率。

## 6. PipeBarrier 使用

阻塞相同流水，具有数据依赖的相同流水之间需要插入此同步。

**示例**:
```cpp
// 同一流水线内部同步
AscendC::PipeBarrier<PIPE_ALL>();
AscendC::PipeBarrier<PIPE_V>();
```